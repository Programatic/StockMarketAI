\title{Using RNN to Predict Stock Market Prices}
\author{Ford Smith}
\date{\today}

\documentclass[12pt]{article}

\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}

\begin{document}
\maketitle

\begin{abstract}
\color{red} //TODO
\end{abstract}

\section{Introduction}
When most people talk about Artificial Intelligence, they are describing machine learning. Machine learning has several difference subsections of tools that can be used to teach a machine from previous experiences by adjusting to inputs to ``perform human-like tasks" (SAS). In this particular experiment, we will be using Neural Networks. Neural Networks are very powerful tools because they grant flexibility to many different scenarios depending on how they are constructed. 


\paragraph{History}
At its core, machine learning is all about taking in inputs and creating a generalized function that can then be extended to predict future values. In fact, we have been doing basic variations of this for hundreds of years: linear regressions. While some function are too difficult to create, we can create line of best fits on data with an input (x) and output (y) to create a function that can approximate or predict a different set of data. See Figure 1. 
%http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/

\begin{figure}[H]
    \centering
    \def\svgwidth{\columnwidth}
    \input{LinearRegression.pdf_tex}
    \caption{An example of linear regression used on input data to create a function to approximate more data}
\end{figure}

This type of learning is called supervised machine learning because through the process of approximating the function, we have input and output data that we can use to make the approximation much more accurate. Unsupervised learning, although not important in this experiment, is learning where don't have output values so we use several different learning methods to learn more about the inputs and how it is structured. \\%https://machinelearningmastery.com/supervised-and-unsupervised-machine-learning-algorithms/

%http://www.andreykurenkov.com/writing/ai/a-brief-history-of-neural-nets-and-deep-learning/
Skip forward about 150 years to the 1950s, we get to the development of the Perceptron. The Perceptron was created by Frank Rosenblatt to be a mathematical model of the neurons in our brains. It takes a set of inputs from nearby Perceptrons, multiplies each of the inputs by a valued weight, and will output a 1 if the weighted inputs reach a threshold, otherwise a 0. This was massive back then because it when put multiple together, it could create basic OR/AND/NOT functions. This was the gateway to formal logic for computers. The most exciting part, however, was that this model could learn by slightly adjusting the weights whenever the output is too low/high. It followed this general order: "
\begin{enumerate}
\item Start off with a Perceptron having random weights and a training set
\item For the inputs of an example in the training set, compute the Perceptronâ€™s output
\item If the output of the Perceptron does not match the output that is known to be correct for the example: If the output should have been 0 but was 1, decrease the weights that had an input of 1. If the output should have been 1 but was 0, increase the weights that had an input of 1.
\item Go to the next example in the training set and repeat steps 2-4 until the Perceptron makes no more mistakes"
\end{enumerate}

This procedure is simple, not terribly computationally heavy, and works best when there is only a limited set of outputs due to thresholds. This is exactly what classification requires - out of a set, accurately determine which output the input data is. Rosenblatt implemented the Perceptrons and inputted 20x20 inputs to accurately classify shapes. Although simple, when many Perceptrons are put together in series called \textbf{layers} the computation power becomes much higher and can work on much more complex data. The same happens when we have multiple layers, where data is put into the input layer, which feeds their output to "hidden" layers who pass their information on to another layer until the information is passed to an output layer. The importance of these hidden layers stems from their ability to find features within the data. Features are essentially specific parts about the data passing through. For instance, if the network was trained on identify whether there was a cat in the image, one of the layers could be used to find if the image has a cat mouth or not while another focuses on finding the left cat ear. This, in turn, allows it to become more accurate. However, there becomes several serious issues when we do this. First, Rosenblatt's method of training does not work with multiple layers. The Perceptrons also can't work on complex features because of the extreme linearity of the weight function inside the Perceptrons. 

%http://neuralnetworksanddeeplearning.com/chap2.html
%http://neuralnetworksanddeeplearning.com/chap1.html
\paragraph{Modern Networks}
\textbf{Backpropagation} was developed across multiple researchers in the 60s and is still commonly used. To break down what backpropagation is, is we have a cost function $C$ with respect to the weight $w$ in the network. We take the partial derivative of $C$ in respect to $w$ to see how quickly the cost changes when we change the weights. The reason backpropagation is so important was because it not only became a much faster learning algorithm, but also tells us detailed information about the change in the system in respect to the weights.  \\

With a newer, faster, and all around better learning algorithm, we were still bottle necked by the actual perceptron itself. When we have some tiny change in its weights, it could cause a drastic change in the output, which actually makes training more difficult. If we can more finely control the weights to have more fine control over the output, we could more accurately train layers to get the results we want. This is where the sigmoid function comes in handy. If we replace the threshold with the sigmoid function, as pictured below, we will then be able more finely change the outputs with the inputs, giving us a higher degree of accuracy.

\begin{figure}[H]
    \centering
    \def\svgwidth{\columnwidth}
    \input{Sigmoid.pdf_tex}
    \caption{An example of a sigmoid function}
\end{figure}

You may be wondering, why use the sigmoid function instead of just having the threshold? Well, the threshold is very linear and doesn't give much opportunity to learn around the data. When the \textbf{activation function} is non-linear (like the sigmoid function) it allows the network as a whole to better adapt to the data. Note, the activation function is what is used to determine the output of the nodes. Here is an image of what a complete Neural Network will use as Figure 3.

%https://medium.com/@curiousily/tensorflow-for-hackers-part-iv-neural-network-from-scratch-1a4f504dfa8
\begin{figure}[H]
  \includegraphics[width=\linewidth]{images/nn.png}
  \caption{A neural network}
\end{figure}

\section{Previous work}\label{previous work}
A much longer \LaTeXe{} example was written by Gil~\cite{Gil:02}.

\section{Results}\label{results}
In this section we describe the results.

\section{Conclusions}\label{conclusions}
We worked hard, and achieved very little.

\bibliographystyle{abbrv}
\bibliography{main}

%https://www.sas.com/en_us/insights/analytics/what-is-artificial-intelligence.html

\end{document}
